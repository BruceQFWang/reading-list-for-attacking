# Reading list for attacking

**[1]**   Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey. 英文综述 [link](https://arxiv.org/pdf/1801.00553.pdf)   

**[2]**   知乎专栏，前几章有介绍样本对抗攻防的基础的 [link](https://zhuanlan.zhihu.com/c_170476465) 

**[3]**   pytorch官方文档----60分钟入门  [link](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) 

**[4]**   样本对抗的来龙去脉和本质  [link](http://baijiahao.baidu.com/s?id=1596201339578975526&wfr=spider&for=pc) 

**[5]**   样本对抗的一个相关比赛  [link](https://tianchi.aliyun.com/competition/entrance/231701/introduction?spm=5176.12281957.1004.2.38b04c2aac5bGR) 

**[6]**   Awesome ML Attack  [link](https://github.com/yenchenlin/awesome-adversarial-machine-learning) 

**[7]**   简单易懂的人脸识别过程和原理介绍 [link](https://blog.csdn.net/LEON1741/article/details/81358974) 

**[8]**   一种鲁棒的神经网络架构(防御) [link](https://arxiv.org/abs/1802.07896)

**[9]**   对抗训练论文一(防御) [link](https://arxiv.org/abs/1805.04807)  

**[10]**  Ian GoodFellow机器学习的博客 [link](http://www.cleverhans.io/?source=post_page---------------------------)


# Open Source about ADVERSARIAL EXAMPLE GENERATION

**[1]** PyTorch FGSM Tutorial [link](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)

**[2]** PyTorch C&W Attack [link](https://github.com/rwightman/pytorch-nips2017-attack-example)

**[3]** PyTorch DDN Attack(CVPR2019) [link](https://github.com/jeromerony/fast_adversarial)

# Face Recognition

**[1]** Loss Function for training Face Recognition Model [link](https://zhuanlan.zhihu.com/p/34404607)

**[2]** Face Recognition Model: ZhaoJ9014/face.evoLVe.PyTorch （默认白盒模型） [link](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)

**[3]** Face Recognition Model: ageitgey/face_recognition （第一次老师给的白盒模型） [link](https://github.com/ageitgey/face_recognition)



# Neural network backdoor

**[0]**  浙大的一篇调研 [link](https://nesa.zju.edu.cn/download/%E9%9D%A2%E5%90%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1by%E5%87%8C%E7%A5%A5%E7%AD%89.pdf)

**[1]** Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. [link](http://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)   翻译 [link](https://blog.csdn.net/qq_38232598/article/details/89244310)

**[2]** Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.(key pattern) [link](https://arxiv.org/pdf/1712.05526.pdf)

**[3]** A General Framework for Adversarial Examples with Objectives.(AGN方法) [link](https://arxiv.org/pdf/1801.00349.pdf)  机器之心的解读[link](https://www.jiqizhixin.com/articles/2018-01-08-5)

**[4]** Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. [link](https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf)   源码[link](https://github.com/mahmoods01/accessorize-to-a-crime)

**[5]** Robust Physical-World Attacks on Deep Learning Visual Classification.(对路牌攻击) [link](https://arxiv.org/pdf/1707.08945.pdf)

# Black-box Attacks  
**[0]** ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models [link](https://arxiv.org/pdf/1708.03999.pdf)  

**[1]** AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks [link](https://arxiv.org/pdf/1805.11770.pdf)  

**[2]** Query-limited Black-box Attacks to Classifiers [link](https://arxiv.org/pdf/1712.08713.pdf)  

**[3]** Delving into Transferable Adversarial Examples and Black-box Attacks [link](https://arxiv.org/pdf/1611.02770.pdf)

**[4]** Ensemble Adversarial Training Attacks and Defenses [link](https://arxiv.org/pdf/1705.07204.pdf)
